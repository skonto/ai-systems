# AIâ€‘Systems ğŸš€
Sample agentic AI projects & reference implementations â€” Retrieval-Augmented Generation (RAG), agents, evaluation pipelines, and more.

## ğŸ“‘ Contents

- [Quickstart](#-quickstart)
- [Architecture](#-architecture)
- [Components](#-components)
- [Evaluation](#-evaluation)
- [Contributing](#-contributing)
- [License](#-license)

## ğŸ”§ Quickstart

```
# Download QA data in txt format under some path eg. data/fantastic_charge
# Save in a file named <filename>.txt.

Disclaimer: The sample data in this repo are generated automatically by an LLM for a fictional product and meant to have no relation with an existing product. Any resemblance is purely coincidental.

# Install Ollama and pull the models
ollama pull llama3:8b-instruct-q4_0
olllama pull BGE-M3:latest

# start opik as shown in the next section

uv sync --dev

# Ingest txt Q&A data in Chroma DB
uv run src/data/rag_ingest.py ./data/products /tmp/ch_db

# Run the Chat Streamlit app
uv run python -m streamlit run ./src/chatbot/app.py

# or
source .venv/bin/activate
python -m streamlit run ./src/chatbot/app.py
```

### Running opik.

Follow the instructions in https://www.comet.com/docs/opik/self-host/local_deployment.
The chatbot app is configured to bypass opik url input and work with opik running locally.


### Interacting with the Q&A Assistant

Access the app at: http://localhost:8501

![ui](./ui.png)


### Building the images

Under deployment you can find several images on how to run the LLMs used in this project.
You can also use the vLLM setup for a production on premise setup.

To build the images:

```
docker build -t skonto/ollama:qa -f Dockerfile.ollama .

docker build --no-cache --progress=plain --secret "id=guard,src=$HOME/.guardrailsrc" . -t skonto/qa
```

To run the images:

```
docker run --gpus all -p8080:11434 skonto/ollama:qa

docker run -it --gpus all -e OLLAMA_HOST=localhost:8080 --net=host skonto/qa
```

## ğŸ§  Architecture

The system consists of:
- Document ingestion and embedding
- RAG pipeline using Ollama
- Chatbot (CLI/HTTP interface)
- Evaluation using RAGAS and custom metrics

The goal is to have a customizable architecture on top of the following rough diagram of the RAG pipeline:

```text
RAG (Retrieval-Augmented Generation) Pipeline
==============================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Metadata  â”‚    â”‚   User Query     â”‚
â”‚   (context,     â”‚    â”‚                  â”‚
â”‚   preferences,  â”‚    â”‚                  â”‚
â”‚   permissions)  â”‚    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚ Query Sanitizerâ”‚
          â”‚              â”‚ & Guardrails   â”‚
          â”‚              â”‚ (input filter) â”‚
          â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â”‚                      â–¼
          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      RETRIEVER        â”‚
                         â”‚                       â”‚
                         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â”‚ â”‚  Search Strategy:   â”‚
                         â”‚ â”‚  â€¢ Hybrid Search    â”‚
                         â”‚ â”‚  â€¢ BM25 (keyword)   â”‚
                         â”‚ â”‚  â€¢ Semantic (vector)â”‚
                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Retrieved Docs     â”‚
                         â”‚   (initial results)   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     RE-RANKER         â”‚
                         â”‚  (relevance scoring   â”‚
                         â”‚   & result ordering)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Top-K Docs        â”‚
                         â”‚   (best matches)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Context Enhancement  â”‚
                         â”‚ (combine query +      â”‚
                         â”‚  retrieved docs +     â”‚
                         â”‚  metadata context)    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚        LLM            â”‚
                         â”‚                       â”‚
                         â”‚ Config:               â”‚
                         â”‚ â€¢ Temperature: 0.7    â”‚
                         â”‚ â€¢ Max tokens: 2048    â”‚
                         â”‚ â€¢ System prompt       â”‚
                         â”‚ â€¢ Model: GPT-4        â”‚
                         â”‚                       â”‚
                         â”‚ Output Format:        â”‚
                         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                         â”‚ â”‚Streamingâ”‚  Chat   â”‚ â”‚
                         â”‚ â”‚ Format  â”‚ Format  â”‚ â”‚
                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Sampling Techniques  â”‚
                         â”‚                       â”‚
                         â”‚ â€¢ Top-p (nucleus)     â”‚
                         â”‚ â€¢ Top-k filtering     â”‚
                         â”‚ â€¢ Temperature scaling â”‚
                         â”‚ â€¢ Repetition penalty  â”‚
                         â”‚ â€¢ Beam search         â”‚
                         â”‚ â€¢ Greedy decoding     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Output Guardrails    â”‚
                         â”‚ (safety filter,       â”‚
                         â”‚  content validation,  â”‚
                         â”‚  bias detection)      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Final Response      â”‚
                         â”‚   (to user)           â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data Flow Legend:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  = Processing step/component
â–¼  = Data flow direction
â”Œâ”€ = Component boundary
â”¤  = Internal component section

Data Flow Legend:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  = Processing step/component
â–¼  = Data flow direction
â”Œâ”€ = Component boundary
â”¤  = Internal component section
```

## ğŸ“¦ Components

- `src/data/`: Document ingestion & indexing
- `src/rag/`: RAG pipelines using Ollama
- `src/chatbot/`: CLI/HTTP chatbot interfaces
- `src/test/`: Unit + integration tests and benchmarks
- `Makefile`, `uv`: commands for lint, test, format, eval

## ğŸ“ˆ Evaluation

Evaluate generated answers using [RAGAS](https://github.com/explodinggradients/ragas):

```bash
uv run pytest -m integration
```

Metrics include:
- `LLMContextRecall`
- `FactualCorrectness`
- `BleuScore`
- `ResponseRelevancy`

## ğŸ¤ Contributing

Contributions welcome!

1. Fork and clone
2. Run `uv sync --dev`
3. Lint: `make lint`
4. Type-check: `make type-check`
5. Test: `pytest`

## ğŸ“„ License

This repository is licensed under the Apache 2 License â€” see [LICENSE](LICENSE) for full details.
